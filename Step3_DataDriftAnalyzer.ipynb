{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b322af0-86db-478a-b2fa-31cb051443fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TEST DATA DRIFT ANALYSIS\n",
      "==================================================\n",
      "============================================================\n",
      "ANALYZING DRIFT BETWEEN CHUNK MODELS AND TEST DATA\n",
      "============================================================\n",
      "Loaded test dataset from: train_test_data/test_data.csv\n",
      "Test dataset shape: (55, 12)\n",
      "Loading chunk models...\n",
      "Loaded logistic_regression chunk 08\n",
      "Loaded logistic_regression chunk 9\n",
      "Loaded logistic_regression chunk 4\n",
      "Loaded logistic_regression chunk 04\n",
      "Loaded logistic_regression chunk 05\n",
      "Loaded logistic_regression chunk 5\n",
      "Loaded logistic_regression chunk 15\n",
      "Loaded logistic_regression chunk 3\n",
      "Loaded logistic_regression chunk 06\n",
      "Loaded logistic_regression chunk 8\n",
      "Loaded logistic_regression chunk 09\n",
      "Loaded logistic_regression chunk 13\n",
      "Loaded logistic_regression chunk 2\n",
      "Loaded logistic_regression chunk 07\n",
      "Loaded logistic_regression chunk 03\n",
      "Loaded logistic_regression chunk 10\n",
      "Loaded logistic_regression chunk 11\n",
      "Loaded logistic_regression chunk 14\n",
      "Loaded logistic_regression chunk 01\n",
      "Loaded logistic_regression chunk 1\n",
      "Loaded logistic_regression chunk 12\n",
      "Loaded logistic_regression chunk 6\n",
      "Loaded logistic_regression chunk 7\n",
      "Loaded logistic_regression chunk 02\n",
      "Loaded decision_tree chunk 08\n",
      "Loaded decision_tree chunk 9\n",
      "Loaded decision_tree chunk 4\n",
      "Loaded decision_tree chunk 04\n",
      "Loaded decision_tree chunk 05\n",
      "Loaded decision_tree chunk 5\n",
      "Loaded decision_tree chunk 15\n",
      "Loaded decision_tree chunk 3\n",
      "Loaded decision_tree chunk 06\n",
      "Loaded decision_tree chunk 8\n",
      "Loaded decision_tree chunk 09\n",
      "Loaded decision_tree chunk 13\n",
      "Loaded decision_tree chunk 2\n",
      "Loaded decision_tree chunk 07\n",
      "Loaded decision_tree chunk 03\n",
      "Loaded decision_tree chunk 10\n",
      "Loaded decision_tree chunk 11\n",
      "Loaded decision_tree chunk 14\n",
      "Loaded decision_tree chunk 01\n",
      "Loaded decision_tree chunk 1\n",
      "Loaded decision_tree chunk 12\n",
      "Loaded decision_tree chunk 6\n",
      "Loaded decision_tree chunk 7\n",
      "Loaded decision_tree chunk 02\n",
      "Loaded neural_network chunk 08\n",
      "Loaded neural_network chunk 9\n",
      "Loaded neural_network chunk 4\n",
      "Loaded neural_network chunk 04\n",
      "Loaded neural_network chunk 05\n",
      "Loaded neural_network chunk 5\n",
      "Loaded neural_network chunk 15\n",
      "Loaded neural_network chunk 3\n",
      "Loaded neural_network chunk 06\n",
      "Loaded neural_network chunk 8\n",
      "Loaded neural_network chunk 09\n",
      "Loaded neural_network chunk 13\n",
      "Loaded neural_network chunk 2\n",
      "Loaded neural_network chunk 07\n",
      "Loaded neural_network chunk 03\n",
      "Loaded neural_network chunk 10\n",
      "Loaded neural_network chunk 11\n",
      "Loaded neural_network chunk 14\n",
      "Loaded neural_network chunk 01\n",
      "Loaded neural_network chunk 1\n",
      "Loaded neural_network chunk 12\n",
      "Loaded neural_network chunk 6\n",
      "Loaded neural_network chunk 7\n",
      "Loaded neural_network chunk 02\n",
      "Loaded svm chunk 08\n",
      "Loaded svm chunk 9\n",
      "Loaded svm chunk 4\n",
      "Loaded svm chunk 04\n",
      "Loaded svm chunk 05\n",
      "Loaded svm chunk 5\n",
      "Loaded svm chunk 15\n",
      "Loaded svm chunk 3\n",
      "Loaded svm chunk 06\n",
      "Loaded svm chunk 8\n",
      "Loaded svm chunk 09\n",
      "Loaded svm chunk 13\n",
      "Loaded svm chunk 2\n",
      "Loaded svm chunk 07\n",
      "Loaded svm chunk 03\n",
      "Loaded svm chunk 10\n",
      "Loaded svm chunk 11\n",
      "Loaded svm chunk 14\n",
      "Loaded svm chunk 01\n",
      "Loaded svm chunk 1\n",
      "Loaded svm chunk 12\n",
      "Loaded svm chunk 6\n",
      "Loaded svm chunk 7\n",
      "Loaded svm chunk 02\n",
      "Loaded lasso_regression chunk 08\n",
      "Loaded lasso_regression chunk 9\n",
      "Loaded lasso_regression chunk 4\n",
      "Loaded lasso_regression chunk 04\n",
      "Loaded lasso_regression chunk 05\n",
      "Loaded lasso_regression chunk 5\n",
      "Loaded lasso_regression chunk 15\n",
      "Loaded lasso_regression chunk 3\n",
      "Loaded lasso_regression chunk 06\n",
      "Loaded lasso_regression chunk 8\n",
      "Loaded lasso_regression chunk 09\n",
      "Loaded lasso_regression chunk 13\n",
      "Loaded lasso_regression chunk 2\n",
      "Loaded lasso_regression chunk 07\n",
      "Loaded lasso_regression chunk 03\n",
      "Loaded lasso_regression chunk 10\n",
      "Loaded lasso_regression chunk 11\n",
      "Loaded lasso_regression chunk 14\n",
      "Loaded lasso_regression chunk 01\n",
      "Loaded lasso_regression chunk 1\n",
      "Loaded lasso_regression chunk 12\n",
      "Loaded lasso_regression chunk 6\n",
      "Loaded lasso_regression chunk 7\n",
      "Loaded lasso_regression chunk 02\n",
      "logistic_regression: 24 chunks loaded\n",
      "decision_tree: 24 chunks loaded\n",
      "neural_network: 24 chunks loaded\n",
      "svm: 24 chunks loaded\n",
      "lasso_regression: 24 chunks loaded\n",
      "Found vectorizer and label encoder in logistic_regression/chunk_9.pkl\n",
      "Using text column: 'full_summary'\n",
      "Using label column: 'industry'\n",
      "Test features shape: (55, 1000)\n",
      "Test labels shape: (55,)\n",
      "\n",
      "Test data size: 55\n",
      "Number of features: 1000\n",
      "Number of classes: 16\n",
      "\n",
      "Analyzing logistic_regression...\n",
      "  Chunk 08: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 9: Data: 0.0941 (DRIFT) | Concept: 0.5818 (DRIFT)\n",
      "  Chunk 4: Data: 0.0952 (DRIFT) | Concept: 0.6545 (DRIFT)\n",
      "  Chunk 04: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 05: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 5: Data: 0.0842 (DRIFT) | Concept: 0.6000 (DRIFT)\n",
      "  Chunk 15: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 3: Data: 0.1008 (DRIFT) | Concept: 0.7273 (DRIFT)\n",
      "  Chunk 06: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 8: Data: 0.1042 (DRIFT) | Concept: 0.6182 (DRIFT)\n",
      "  Chunk 09: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 13: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 2: Data: 0.1047 (DRIFT) | Concept: 0.7273 (DRIFT)\n",
      "  Chunk 07: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 03: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 10: Data: 0.0958 (DRIFT) | Concept: 0.7091 (DRIFT)\n",
      "  Chunk 11: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 14: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 01: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 1: Data: 0.0942 (DRIFT) | Concept: 0.5818 (DRIFT)\n",
      "  Chunk 12: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 6: Data: 0.0976 (DRIFT) | Concept: 0.6364 (DRIFT)\n",
      "  Chunk 7: Data: 0.0900 (DRIFT) | Concept: 0.6727 (DRIFT)\n",
      "  Chunk 02: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "\n",
      "Analyzing decision_tree...\n",
      "  Chunk 08: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 9: Data: 0.0948 (DRIFT) | Concept: 0.4667 (DRIFT)\n",
      "  Chunk 4: Data: 0.0954 (DRIFT) | Concept: 0.5226 (DRIFT)\n",
      "  Chunk 04: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 05: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 5: Data: 0.0904 (DRIFT) | Concept: 0.5973 (DRIFT)\n",
      "  Chunk 15: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 3: Data: 0.1018 (DRIFT) | Concept: 0.7818 (DRIFT)\n",
      "  Chunk 06: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 8: Data: 0.1046 (DRIFT) | Concept: 0.5226 (DRIFT)\n",
      "  Chunk 09: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 13: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 2: Data: 0.1106 (DRIFT) | Concept: 0.7273 (DRIFT)\n",
      "  Chunk 07: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 03: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 10: Data: 0.1012 (DRIFT) | Concept: 0.8182 (DRIFT)\n",
      "  Chunk 11: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 14: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 01: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 1: Data: 0.0951 (DRIFT) | Concept: 0.6571 (DRIFT)\n",
      "  Chunk 12: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 6: Data: 0.0903 (DRIFT) | Concept: 0.5414 (DRIFT)\n",
      "  Chunk 7: Data: 0.0908 (DRIFT) | Concept: 0.6889 (DRIFT)\n",
      "  Chunk 02: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "\n",
      "Analyzing neural_network...\n",
      "  Chunk 08: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 9: Data: 0.0950 (DRIFT) | Concept: 0.0000 (OK)\n",
      "  Chunk 4: Data: 0.0964 (DRIFT) | Concept: 0.0195 (DRIFT)\n",
      "  Chunk 04: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 05: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 5: Data: 0.0830 (DRIFT) | Concept: 0.0566 (DRIFT)\n",
      "  Chunk 15: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 3: Data: 0.0991 (DRIFT) | Concept: 0.0013 (OK)\n",
      "  Chunk 06: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 8: Data: 0.0994 (DRIFT) | Concept: 0.0566 (DRIFT)\n",
      "  Chunk 09: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 13: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 2: Data: 0.1125 (DRIFT) | Concept: 0.0000 (OK)\n",
      "  Chunk 07: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 03: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 10: Data: 0.0956 (DRIFT) | Concept: 0.0202 (DRIFT)\n",
      "  Chunk 11: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 14: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 01: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 1: Data: 0.0929 (DRIFT) | Concept: 0.0169 (DRIFT)\n",
      "  Chunk 12: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 6: Data: 0.0920 (DRIFT) | Concept: 0.0384 (DRIFT)\n",
      "  Chunk 7: Data: 0.0930 (DRIFT) | Concept: 0.0013 (OK)\n",
      "  Chunk 02: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "\n",
      "Analyzing svm...\n",
      "  Chunk 08: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 9: Data: 0.0981 (DRIFT) | Concept: 0.6364 (DRIFT)\n",
      "  Chunk 4: Data: 0.0907 (DRIFT) | Concept: 0.6343 (DRIFT)\n",
      "  Chunk 04: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 05: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 5: Data: 0.0874 (DRIFT) | Concept: 0.6714 (DRIFT)\n",
      "  Chunk 15: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 3: Data: 0.0995 (DRIFT) | Concept: 0.6909 (DRIFT)\n",
      "  Chunk 06: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 8: Data: 0.1004 (DRIFT) | Concept: 0.7091 (DRIFT)\n",
      "  Chunk 09: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 13: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 2: Data: 0.1083 (DRIFT) | Concept: 0.7455 (DRIFT)\n",
      "  Chunk 07: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 03: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 10: Data: 0.0990 (DRIFT) | Concept: 0.7273 (DRIFT)\n",
      "  Chunk 11: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 14: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 01: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 1: Data: 0.0977 (DRIFT) | Concept: 0.6545 (DRIFT)\n",
      "  Chunk 12: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 6: Data: 0.0975 (DRIFT) | Concept: 0.7273 (DRIFT)\n",
      "  Chunk 7: Data: 0.0863 (DRIFT) | Concept: 0.7084 (DRIFT)\n",
      "  Chunk 02: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "\n",
      "Analyzing lasso_regression...\n",
      "  Chunk 08: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 9: Data: 0.0873 (DRIFT) | Concept: 0.0000 (OK)\n",
      "  Chunk 4: Data: 0.0905 (DRIFT) | Concept: 0.0202 (DRIFT)\n",
      "  Chunk 04: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 05: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 5: Data: 0.0858 (DRIFT) | Concept: 0.0384 (DRIFT)\n",
      "  Chunk 15: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 3: Data: 0.0975 (DRIFT) | Concept: 0.0000 (OK)\n",
      "  Chunk 06: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 8: Data: 0.1077 (DRIFT) | Concept: 0.0384 (DRIFT)\n",
      "  Chunk 09: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 13: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 2: Data: 0.1110 (DRIFT) | Concept: 0.0526 (DRIFT)\n",
      "  Chunk 07: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 03: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 10: Data: 0.0968 (DRIFT) | Concept: 0.0384 (DRIFT)\n",
      "  Chunk 11: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 14: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 01: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 1: Data: 0.0948 (DRIFT) | Concept: 0.0000 (OK)\n",
      "  Chunk 12: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "  Chunk 6: Data: 0.0894 (DRIFT) | Concept: 0.0000 (OK)\n",
      "  Chunk 7: Data: 0.0874 (DRIFT) | Concept: 0.0000 (OK)\n",
      "  Chunk 02: No training data in chunk, skipping data drift\n",
      "Data: N/A | Concept: N/A\n",
      "\n",
      "======================================================================\n",
      "DRIFT ANALYSIS SUMMARY - TEST DATA\n",
      "======================================================================\n",
      "Analysis Timestamp: 2025-10-11T05:53:37.049757\n",
      "Test Data Size: 55\n",
      "Total Chunks Analyzed: 120\n",
      "\n",
      "DATA DRIFT SUMMARY:\n",
      "  Mean Data Drift Score: 0.0962\n",
      "  Max Data Drift Score:  0.1125\n",
      "  Min Data Drift Score:  0.0830\n",
      "  Drift Detected in:     50 out of 50 comparisons\n",
      "\n",
      "CONCEPT DRIFT SUMMARY BY MODEL:\n",
      "  logistic_regression  | Mean: 0.6509 | Max: 0.7273 | Drifts: 10/10\n",
      "  decision_tree        | Mean: 0.6324 | Max: 0.8182 | Drifts: 10/10\n",
      "  neural_network       | Mean: 0.0211 | Max: 0.0566 | Drifts: 6/10\n",
      "  svm                  | Mean: 0.6905 | Max: 0.7455 | Drifts: 10/10\n",
      "  lasso_regression     | Mean: 0.0188 | Max: 0.0526 | Drifts: 5/10\n",
      "\n",
      "Drift results saved to: drift_reports/test_data_drift_report.pkl\n",
      "\n",
      "ANALYSIS COMPLETED SUCCESSFULLY\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class TestDataDriftAnalyzer:\n",
    "    def __init__(self, config_path=\"config.yaml\"):\n",
    "        self.config = self.load_config(config_path)\n",
    "        self.test_df = None\n",
    "        self.chunk_models = {}\n",
    "        self.drift_results = {}\n",
    "        self.vectorizer = None\n",
    "        self.label_encoder = None\n",
    "        \n",
    "    def load_config(self, config_path):\n",
    "        \"\"\"Load configuration\"\"\"\n",
    "        try:\n",
    "            import yaml\n",
    "            with open(config_path, 'r') as file:\n",
    "                return yaml.safe_load(file)\n",
    "        except:\n",
    "            return self.get_default_config()\n",
    "    \n",
    "    def get_default_config(self):\n",
    "        \"\"\"Default configuration\"\"\"\n",
    "        return {\n",
    "            'paths': {\n",
    "                'test_dataset': \"balanced_dataset/test_data.csv\",\n",
    "                'models_dir': \"models\",\n",
    "                'drift_reports_dir': \"drift_reports\",\n",
    "                'drift_file': \"test_data_drift_report.pkl\"\n",
    "            },\n",
    "            'drift_detection': {\n",
    "                'data_drift_threshold': 0.01,\n",
    "                'concept_drift_threshold': 0.006,\n",
    "                'significance_level': 0.006\n",
    "            },\n",
    "            'models': ['logistic_regression', 'decision_tree', 'neural_network', 'svm', 'lasso_regression']\n",
    "        }\n",
    "    \n",
    "    def load_test_data(self):\n",
    "        \"\"\"Load test dataset\"\"\"\n",
    "        test_path = self.config['paths']['test_dataset']\n",
    "        possible_paths = [\n",
    "            test_path,\n",
    "            os.path.join(\"balanced_dataset\", \"test_data.csv\"),\n",
    "            os.path.join(\"balanced_dataset\", \"test_data.pkl\"),\n",
    "            \"test_data.csv\"\n",
    "        ]\n",
    "        \n",
    "        for data_path in possible_paths:\n",
    "            try:\n",
    "                if os.path.exists(data_path):\n",
    "                    if data_path.endswith('.pkl'):\n",
    "                        with open(data_path, 'rb') as f:\n",
    "                            self.test_df = pickle.load(f)\n",
    "                    else:\n",
    "                        self.test_df = pd.read_csv(data_path)\n",
    "                    \n",
    "                    print(f\"Loaded test dataset from: {data_path}\")\n",
    "                    print(f\"Test dataset shape: {self.test_df.shape}\")\n",
    "                    return True\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load from {data_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(\"Could not load test dataset\")\n",
    "        return False\n",
    "    \n",
    "    def auto_detect_columns(self):\n",
    "        \"\"\"Auto-detect text and label columns\"\"\"\n",
    "        if self.test_df is None:\n",
    "            return None, None\n",
    "        \n",
    "        # Label column candidates\n",
    "        label_candidates = ['label', 'category', 'class', 'target', 'industry', 'industry_name']\n",
    "        for candidate in label_candidates:\n",
    "            if candidate in self.test_df.columns:\n",
    "                label_col = candidate\n",
    "                break\n",
    "        else:\n",
    "            for col in self.test_df.columns:\n",
    "                if self.test_df[col].dtype == 'object' and 2 <= self.test_df[col].nunique() <= 50:\n",
    "                    label_col = col\n",
    "                    break\n",
    "            else:\n",
    "                label_col = self.test_df.columns[0] if len(self.test_df.columns) > 0 else None\n",
    "        \n",
    "        # Text column candidates\n",
    "        text_candidates = ['text', 'content', 'summary', 'description', 'full_summary']\n",
    "        for candidate in text_candidates:\n",
    "            if candidate in self.test_df.columns:\n",
    "                text_col = candidate\n",
    "                break\n",
    "        else:\n",
    "            for col in self.test_df.columns:\n",
    "                if self.test_df[col].dtype == 'object':\n",
    "                    avg_length = self.test_df[col].astype(str).str.len().mean()\n",
    "                    if avg_length > 10:\n",
    "                        text_col = col\n",
    "                        break\n",
    "            else:\n",
    "                text_col = self.test_df.columns[0] if len(self.test_df.columns) > 0 else None\n",
    "        \n",
    "        return text_col, label_col\n",
    "    \n",
    "    def load_chunk_models(self):\n",
    "        \"\"\"Load all chunk models from models directory\"\"\"\n",
    "        models_dir = self.config['paths']['models_dir']\n",
    "        models = self.config['models']\n",
    "        \n",
    "        print(\"Loading chunk models...\")\n",
    "        \n",
    "        for model_name in models:\n",
    "            self.chunk_models[model_name] = {}\n",
    "            model_dir = os.path.join(models_dir, model_name)\n",
    "            \n",
    "            if not os.path.exists(model_dir):\n",
    "                print(f\"Model directory not found: {model_dir}\")\n",
    "                continue\n",
    "            \n",
    "            # Load all chunk model files\n",
    "            for filename in os.listdir(model_dir):\n",
    "                if filename.startswith('chunk_') and filename.endswith('.pkl'):\n",
    "                    try:\n",
    "                        chunk_id = filename.replace('chunk_', '').replace('.pkl', '')\n",
    "                        filepath = os.path.join(model_dir, filename)\n",
    "                        \n",
    "                        with open(filepath, 'rb') as f:\n",
    "                            model_data = pickle.load(f)\n",
    "                        \n",
    "                        self.chunk_models[model_name][chunk_id] = model_data\n",
    "                        print(f\"Loaded {model_name} chunk {chunk_id}\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading {model_name} {filename}: {e}\")\n",
    "        \n",
    "        # Print summary\n",
    "        for model_name, chunks in self.chunk_models.items():\n",
    "            print(f\"{model_name}: {len(chunks)} chunks loaded\")\n",
    "        \n",
    "        return self.chunk_models\n",
    "    \n",
    "    def prepare_test_features(self, vectorizer, label_encoder):\n",
    "        \"\"\"Prepare test features using the same vectorizer and label encoder\"\"\"\n",
    "        text_col, label_col = self.auto_detect_columns()\n",
    "        \n",
    "        if not text_col or not label_col:\n",
    "            print(\"Could not auto-detect text or label columns\")\n",
    "            print(f\"Available columns: {self.test_df.columns.tolist()}\")\n",
    "            return False, None, None\n",
    "        \n",
    "        print(f\"Using text column: '{text_col}'\")\n",
    "        print(f\"Using label column: '{label_col}'\")\n",
    "        \n",
    "        # Prepare text features\n",
    "        texts = self.test_df[text_col].fillna('').astype(str)\n",
    "        X_test = vectorizer.transform(texts)\n",
    "        \n",
    "        # Prepare labels\n",
    "        y_test = self.test_df[label_col]\n",
    "        y_test_encoded = label_encoder.transform(y_test)\n",
    "        \n",
    "        print(f\"Test features shape: {X_test.shape}\")\n",
    "        print(f\"Test labels shape: {y_test_encoded.shape}\")\n",
    "        \n",
    "        return True, X_test, y_test_encoded\n",
    "    \n",
    "    def find_vectorizer_and_encoder(self):\n",
    "        \"\"\"Find vectorizer and label encoder from model files\"\"\"\n",
    "        models_dir = self.config['paths']['models_dir']\n",
    "        \n",
    "        # Check each model directory for files containing vectorizer and encoder\n",
    "        for model_name in self.config['models']:\n",
    "            model_dir = os.path.join(models_dir, model_name)\n",
    "            if not os.path.exists(model_dir):\n",
    "                continue\n",
    "            \n",
    "            for filename in os.listdir(model_dir):\n",
    "                if filename.endswith('.pkl'):\n",
    "                    try:\n",
    "                        filepath = os.path.join(model_dir, filename)\n",
    "                        with open(filepath, 'rb') as f:\n",
    "                            model_data = pickle.load(f)\n",
    "                        \n",
    "                        # Check if this file contains vectorizer and encoder\n",
    "                        if 'vectorizer' in model_data and 'label_encoder' in model_data:\n",
    "                            self.vectorizer = model_data['vectorizer']\n",
    "                            self.label_encoder = model_data['label_encoder']\n",
    "                            print(f\"Found vectorizer and label encoder in {model_name}/{filename}\")\n",
    "                            return True\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "        \n",
    "        # If not found, try to load from overall best model\n",
    "        best_model_path = os.path.join(\"best_models\", \"overall_best_model.pkl\")\n",
    "        if os.path.exists(best_model_path):\n",
    "            try:\n",
    "                with open(best_model_path, 'rb') as f:\n",
    "                    best_model_data = pickle.load(f)\n",
    "                if 'vectorizer' in best_model_data and 'label_encoder' in best_model_data:\n",
    "                    self.vectorizer = best_model_data['vectorizer']\n",
    "                    self.label_encoder = best_model_data['label_encoder']\n",
    "                    print(\"Found vectorizer and label encoder in overall best model\")\n",
    "                    return True\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading overall best model: {e}\")\n",
    "        \n",
    "        print(\"Could not find vectorizer and label encoder\")\n",
    "        return False\n",
    "    \n",
    "    def calculate_data_drift(self, X_train_chunk, X_test):\n",
    "        \"\"\"Calculate data drift between training chunk and test data\"\"\"\n",
    "        try:\n",
    "            # Convert to dense arrays if sparse\n",
    "            X1 = X_train_chunk.toarray() if hasattr(X_train_chunk, 'toarray') else X_train_chunk\n",
    "            X2 = X_test.toarray() if hasattr(X_test, 'toarray') else X_test\n",
    "            \n",
    "            # Sample features for efficiency\n",
    "            n_features = min(X1.shape[1], X2.shape[1], 100)\n",
    "            feature_indices = np.random.choice(min(X1.shape[1], X2.shape[1]), n_features, replace=False)\n",
    "            \n",
    "            # Calculate KS scores\n",
    "            ks_scores = []\n",
    "            for feature_idx in feature_indices:\n",
    "                if feature_idx < X1.shape[1] and feature_idx < X2.shape[1]:\n",
    "                    ks_stat, _ = ks_2samp(X1[:, feature_idx], X2[:, feature_idx])\n",
    "                    ks_scores.append(ks_stat)\n",
    "            \n",
    "            avg_ks_score = np.mean(ks_scores) if ks_scores else 0\n",
    "            \n",
    "            # Calculate cosine distance\n",
    "            mean1 = np.mean(X1, axis=0)\n",
    "            mean2 = np.mean(X2, axis=0)\n",
    "            cosine_sim = cosine_similarity([mean1], [mean2])[0][0]\n",
    "            cosine_distance = 1 - cosine_sim\n",
    "            \n",
    "            # Combined drift score\n",
    "            data_drift_score = (avg_ks_score + cosine_distance) / 2\n",
    "            \n",
    "            drift_threshold = self.config['drift_detection']['data_drift_threshold']\n",
    "            \n",
    "            return {\n",
    "                'data_drift_score': float(data_drift_score),\n",
    "                'ks_score': float(avg_ks_score),\n",
    "                'cosine_distance': float(cosine_distance),\n",
    "                'has_drift': data_drift_score > drift_threshold\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating data drift: {e}\")\n",
    "            return {\n",
    "                'data_drift_score': 1.0,\n",
    "                'ks_score': 1.0,\n",
    "                'cosine_distance': 1.0,\n",
    "                'has_drift': True,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def calculate_concept_drift(self, model, X_train_chunk, y_train_chunk, X_test, y_test):\n",
    "        \"\"\"Calculate concept drift using model performance\"\"\"\n",
    "        try:\n",
    "            # Train model on chunk data\n",
    "            model_copy = model.fit(X_train_chunk, y_train_chunk)\n",
    "            \n",
    "            # Calculate accuracy on training chunk and test data\n",
    "            train_accuracy = accuracy_score(y_train_chunk, model_copy.predict(X_train_chunk))\n",
    "            test_accuracy = accuracy_score(y_test, model_copy.predict(X_test))\n",
    "            \n",
    "            accuracy_drop = train_accuracy - test_accuracy\n",
    "            concept_drift_score = max(0, accuracy_drop)\n",
    "            \n",
    "            drift_threshold = self.config['drift_detection']['concept_drift_threshold']\n",
    "            \n",
    "            return {\n",
    "                'concept_drift_score': float(concept_drift_score),\n",
    "                'accuracy_drop': float(accuracy_drop),\n",
    "                'train_accuracy': float(train_accuracy),\n",
    "                'test_accuracy': float(test_accuracy),\n",
    "                'has_drift': concept_drift_score > drift_threshold\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating concept drift: {e}\")\n",
    "            return {\n",
    "                'concept_drift_score': 1.0,\n",
    "                'accuracy_drop': 1.0,\n",
    "                'train_accuracy': 0.0,\n",
    "                'test_accuracy': 0.0,\n",
    "                'has_drift': True,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def analyze_test_data_drift(self):\n",
    "        \"\"\"Main method to analyze drift between chunk models and test data\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"ANALYZING DRIFT BETWEEN CHUNK MODELS AND TEST DATA\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Load test data\n",
    "        if not self.load_test_data():\n",
    "            return False\n",
    "        \n",
    "        # Load chunk models\n",
    "        self.load_chunk_models()\n",
    "        \n",
    "        # Find vectorizer and label encoder\n",
    "        if not self.find_vectorizer_and_encoder():\n",
    "            print(\"Need vectorizer and label encoder to process test data\")\n",
    "            return False\n",
    "        \n",
    "        # Prepare test features\n",
    "        success, X_test, y_test = self.prepare_test_features(self.vectorizer, self.label_encoder)\n",
    "        if not success:\n",
    "            return False\n",
    "        \n",
    "        # Initialize drift results structure\n",
    "        self.drift_results = {\n",
    "            'data_drift': {},\n",
    "            'concept_drift': {},\n",
    "            'summary': {\n",
    "                'analysis_timestamp': datetime.now().isoformat(),\n",
    "                'test_data_size': len(self.test_df),\n",
    "                'total_chunks_analyzed': 0,\n",
    "                'drift_thresholds': self.config['drift_detection']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nTest data size: {len(self.test_df)}\")\n",
    "        print(f\"Number of features: {X_test.shape[1]}\")\n",
    "        print(f\"Number of classes: {len(np.unique(y_test))}\")\n",
    "        \n",
    "        # Analyze drift for each model and chunk\n",
    "        data_drift_scores = []\n",
    "        concept_drift_scores_by_model = {model: [] for model in self.config['models']}\n",
    "        \n",
    "        for model_name, chunks in self.chunk_models.items():\n",
    "            print(f\"\\nAnalyzing {model_name}...\")\n",
    "            \n",
    "            for chunk_id, chunk_data in chunks.items():\n",
    "                print(f\"  Chunk {chunk_id}: \", end=\"\")\n",
    "                \n",
    "                # Get training data from chunk (if available)\n",
    "                if 'X' in chunk_data and 'y' in chunk_data:\n",
    "                    X_train_chunk = chunk_data['X']\n",
    "                    y_train_chunk = chunk_data['y']\n",
    "                else:\n",
    "                    # If chunk doesn't contain training data, skip data drift\n",
    "                    print(\"No training data in chunk, skipping data drift\")\n",
    "                    X_train_chunk = None\n",
    "                    y_train_chunk = None\n",
    "                \n",
    "                # Calculate data drift\n",
    "                if X_train_chunk is not None:\n",
    "                    data_drift = self.calculate_data_drift(X_train_chunk, X_test)\n",
    "                    drift_key = f\"{model_name}_chunk_{chunk_id}_to_test\"\n",
    "                    self.drift_results['data_drift'][drift_key] = data_drift\n",
    "                    data_drift_scores.append(data_drift['data_drift_score'])\n",
    "                    \n",
    "                    data_status = \"DRIFT\" if data_drift['has_drift'] else \"OK\"\n",
    "                    print(f\"Data: {data_drift['data_drift_score']:.4f} ({data_status})\", end=\"\")\n",
    "                else:\n",
    "                    print(\"Data: N/A\", end=\"\")\n",
    "                \n",
    "                # Calculate concept drift\n",
    "                if 'model' in chunk_data and y_train_chunk is not None:\n",
    "                    concept_drift = self.calculate_concept_drift(\n",
    "                        chunk_data['model'], X_train_chunk, y_train_chunk, X_test, y_test\n",
    "                    )\n",
    "                    drift_key = f\"{model_name}_chunk_{chunk_id}_to_test\"\n",
    "                    self.drift_results['concept_drift'][drift_key] = concept_drift\n",
    "                    concept_drift_scores_by_model[model_name].append(concept_drift['concept_drift_score'])\n",
    "                    \n",
    "                    concept_status = \"DRIFT\" if concept_drift['has_drift'] else \"OK\"\n",
    "                    print(f\" | Concept: {concept_drift['concept_drift_score']:.4f} ({concept_status})\")\n",
    "                else:\n",
    "                    print(\" | Concept: N/A\")\n",
    "        \n",
    "        # Calculate summaries\n",
    "        if data_drift_scores:\n",
    "            self.drift_results['summary']['data_drift'] = {\n",
    "                'mean_score': float(np.mean(data_drift_scores)),\n",
    "                'max_score': float(np.max(data_drift_scores)),\n",
    "                'min_score': float(np.min(data_drift_scores)),\n",
    "                'drift_detected_count': sum(1 for score in data_drift_scores \n",
    "                                          if score > self.config['drift_detection']['data_drift_threshold']),\n",
    "                'total_comparisons': len(data_drift_scores)\n",
    "            }\n",
    "        \n",
    "        # Model-wise concept drift summaries\n",
    "        for model_name, scores in concept_drift_scores_by_model.items():\n",
    "            if scores:\n",
    "                self.drift_results['concept_drift'][f\"{model_name}_summary\"] = {\n",
    "                    'mean_concept_drift': float(np.mean(scores)),\n",
    "                    'max_concept_drift': float(np.max(scores)),\n",
    "                    'min_concept_drift': float(np.min(scores)),\n",
    "                    'drift_detected_count': sum(1 for score in scores \n",
    "                                              if score > self.config['drift_detection']['concept_drift_threshold']),\n",
    "                    'chunks_analyzed': len(scores)\n",
    "                }\n",
    "        \n",
    "        self.drift_results['summary']['total_chunks_analyzed'] = sum(\n",
    "            len(chunks) for chunks in self.chunk_models.values()\n",
    "        )\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def print_drift_summary(self):\n",
    "        \"\"\"Print summary of drift analysis\"\"\"\n",
    "        if not self.drift_results:\n",
    "            print(\"No drift results available\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"DRIFT ANALYSIS SUMMARY - TEST DATA\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        summary = self.drift_results['summary']\n",
    "        \n",
    "        print(f\"Analysis Timestamp: {summary['analysis_timestamp']}\")\n",
    "        print(f\"Test Data Size: {summary['test_data_size']}\")\n",
    "        print(f\"Total Chunks Analyzed: {summary['total_chunks_analyzed']}\")\n",
    "        \n",
    "        if 'data_drift' in summary:\n",
    "            data_summary = summary['data_drift']\n",
    "            print(f\"\\nDATA DRIFT SUMMARY:\")\n",
    "            print(f\"  Mean Data Drift Score: {data_summary['mean_score']:.4f}\")\n",
    "            print(f\"  Max Data Drift Score:  {data_summary['max_score']:.4f}\")\n",
    "            print(f\"  Min Data Drift Score:  {data_summary['min_score']:.4f}\")\n",
    "            print(f\"  Drift Detected in:     {data_summary['drift_detected_count']} out of {data_summary['total_comparisons']} comparisons\")\n",
    "        \n",
    "        print(f\"\\nCONCEPT DRIFT SUMMARY BY MODEL:\")\n",
    "        for model_name in self.config['models']:\n",
    "            model_key = f\"{model_name}_summary\"\n",
    "            if model_key in self.drift_results['concept_drift']:\n",
    "                model_summary = self.drift_results['concept_drift'][model_key]\n",
    "                print(f\"  {model_name:20} | Mean: {model_summary['mean_concept_drift']:.4f} | \"\n",
    "                      f\"Max: {model_summary['max_concept_drift']:.4f} | \"\n",
    "                      f\"Drifts: {model_summary['drift_detected_count']}/{model_summary['chunks_analyzed']}\")\n",
    "    \n",
    "    def save_drift_results(self):\n",
    "        \"\"\"Save drift results to pickle file\"\"\"\n",
    "        drift_reports_dir = self.config['paths']['drift_reports_dir']\n",
    "        os.makedirs(drift_reports_dir, exist_ok=True)\n",
    "        \n",
    "        drift_file = os.path.join(drift_reports_dir, self.config['paths']['drift_file'])\n",
    "        \n",
    "        with open(drift_file, 'wb') as f:\n",
    "            pickle.dump(self.drift_results, f)\n",
    "        \n",
    "        print(f\"\\nDrift results saved to: {drift_file}\")\n",
    "        return drift_file\n",
    "    \n",
    "    def run_analysis(self):\n",
    "        \"\"\"Run complete drift analysis\"\"\"\n",
    "        print(\"STARTING TEST DATA DRIFT ANALYSIS\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        success = self.analyze_test_data_drift()\n",
    "        \n",
    "        if success:\n",
    "            self.print_drift_summary()\n",
    "            self.save_drift_results()\n",
    "            print(\"\\nANALYSIS COMPLETED SUCCESSFULLY\")\n",
    "        else:\n",
    "            print(\"\\nANALYSIS FAILED\")\n",
    "        \n",
    "        return success\n",
    "\n",
    "\n",
    "def main():\n",
    "    analyzer = TestDataDriftAnalyzer(\"config.yaml\")\n",
    "    analyzer.run_analysis()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da64716c-e6ea-4dd6-bfe6-2ddcc29f43ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
