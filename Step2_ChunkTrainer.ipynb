{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "notebook_path = input(\"Enter the path to your .ipynb file directory: \")\n",
        "if not os.path.exists(notebook_path): sys.exit(f\"Path {notebook_path} not found\")\n",
        "os.chdir(notebook_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BhfBvu0dEDQ",
        "outputId": "21db5b2d-c0c2-4b95-8600-a0c0f78d711a"
      },
      "id": "3BhfBvu0dEDQ",
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Enter the path to your .ipynb file directory: /content/drive/MyDrive/LJMU_Data_XAI/TestGItCode/whitebox_mlops/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wbrxJY01Reg8"
      },
      "id": "wbrxJY01Reg8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r7s3axO_eOz9"
      },
      "id": "r7s3axO_eOz9",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3c54dd29-5432-40db-8703-e3776a4064f9",
      "metadata": {
        "id": "3c54dd29-5432-40db-8703-e3776a4064f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "366bae8d-ee55-4aae-fdef-c24f10a492a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration loaded successfully from config.yaml\n",
            "STARTING COMPLETE TRAINING PIPELINE\n",
            "==================================================\n",
            "Loaded balanced dataset from: balanced_dataset/balanced_dataset.csv\n",
            "Dataset shape: (272, 12)\n",
            "Using text column: 'full_summary'\n",
            "Using label column: 'industry'\n",
            "Created features with shape: (272, 1000)\n",
            "Number of classes: 16\n",
            "Classes: ['Automotive', 'Consulting', 'E-commerce', 'Education', 'Energy', 'Finance', 'Government', 'HR', 'Healthcare', 'Insurance', 'Legal', 'Media & Entertainment', 'Other', 'Research & Academia', 'Tech', 'Telecommunications']\n",
            "\n",
            "Creating 10 chunks from 272 samples...\n",
            "Chunk 1: 28 samples, 15 classes\n",
            "Chunk 2: 28 samples, 12 classes\n",
            "Chunk 3: 27 samples, 13 classes\n",
            "Chunk 4: 27 samples, 13 classes\n",
            "Chunk 5: 27 samples, 14 classes\n",
            "Chunk 6: 27 samples, 14 classes\n",
            "Chunk 7: 27 samples, 13 classes\n",
            "Chunk 8: 27 samples, 14 classes\n",
            "Chunk 9: 27 samples, 15 classes\n",
            "Chunk 10: 27 samples, 13 classes\n",
            "Initialized 5 models for multiclass classification:\n",
            "  - logistic_regression\n",
            "  - decision_tree\n",
            "  - neural_network\n",
            "  - svm\n",
            "  - lasso_regression\n",
            "\n",
            "============================================================\n",
            "TRAINING ALL 5 MODELS ON CHUNK 1\n",
            "============================================================\n",
            "\n",
            "--- Training LOGISTIC_REGRESSION on chunk 1 ---\n",
            "  SUCCESS - Accuracy: 1.0000, F1: 1.0000\n",
            "  NEW BEST - Best chunk for logistic_regression: 1 (f1_score: 1.0000)\n",
            "\n",
            "--- Training DECISION_TREE on chunk 1 ---\n",
            "  SUCCESS - Accuracy: 0.6786, F1: 0.6143\n",
            "  NEW BEST - Best chunk for decision_tree: 1 (f1_score: 0.6143)\n",
            "\n",
            "--- Training NEURAL_NETWORK on chunk 1 ---\n",
            "  SUCCESS - Accuracy: 0.0714, F1: 0.0099\n",
            "  NEW BEST - Best chunk for neural_network: 1 (f1_score: 0.0099)\n",
            "\n",
            "--- Training SVM on chunk 1 ---\n",
            "  SUCCESS - Accuracy: 1.0000, F1: 1.0000\n",
            "  NEW BEST - Best chunk for svm: 1 (f1_score: 1.0000)\n",
            "\n",
            "--- Training LASSO_REGRESSION on chunk 1 ---\n",
            "  SUCCESS - Accuracy: 0.0357, F1: 0.0025\n",
            "  NEW BEST - Best chunk for lasso_regression: 1 (f1_score: 0.0025)\n",
            "\n",
            "============================================================\n",
            "TRAINING ALL 5 MODELS ON CHUNK 2\n",
            "============================================================\n",
            "\n",
            "--- Training LOGISTIC_REGRESSION on chunk 2 ---\n",
            "  SUCCESS - Accuracy: 1.0000, F1: 1.0000\n",
            "\n",
            "--- Training DECISION_TREE on chunk 2 ---\n",
            "  SUCCESS - Accuracy: 1.0000, F1: 1.0000\n",
            "  NEW BEST - Best chunk for decision_tree: 2 (f1_score: 1.0000)\n",
            "\n",
            "--- Training NEURAL_NETWORK on chunk 2 ---\n",
            "  SUCCESS - Accuracy: 0.0357, F1: 0.0026\n",
            "\n",
            "--- Training SVM on chunk 2 ---\n",
            "  SUCCESS - Accuracy: 1.0000, F1: 1.0000\n",
            "\n",
            "--- Training LASSO_REGRESSION on chunk 2 ---\n",
            "  SUCCESS - Accuracy: 0.1071, F1: 0.0741\n",
            "  NEW BEST - Best chunk for lasso_regression: 2 (f1_score: 0.0741)\n",
            "\n",
            "============================================================\n",
            "TRAINING ALL 5 MODELS ON CHUNK 3\n",
            "============================================================\n",
            "\n",
            "--- Training LOGISTIC_REGRESSION on chunk 3 ---\n",
            "  SUCCESS - Accuracy: 1.0000, F1: 1.0000\n",
            "\n",
            "--- Training DECISION_TREE on chunk 3 ---\n",
            "  SUCCESS - Accuracy: 0.8889, F1: 0.8519\n",
            "\n",
            "--- Training NEURAL_NETWORK on chunk 3 ---\n",
            "  SUCCESS - Accuracy: 0.0741, F1: 0.0102\n",
            "  NEW BEST - Best chunk for neural_network: 3 (f1_score: 0.0102)\n",
            "\n",
            "--- Training SVM on chunk 3 ---\n",
            "  SUCCESS - Accuracy: 1.0000, F1: 1.0000\n",
            "\n",
            "--- Training LASSO_REGRESSION on chunk 3 ---\n",
            "  SUCCESS - Accuracy: 0.0370, F1: 0.0026\n",
            "\n",
            "============================================================\n",
            "TRAINING ALL 5 MODELS ON CHUNK 4\n",
            "============================================================\n",
            "\n",
            "--- Training LOGISTIC_REGRESSION on chunk 4 ---\n",
            "  SUCCESS - Accuracy: 1.0000, F1: 1.0000\n",
            "\n",
            "--- Training DECISION_TREE on chunk 4 ---\n",
            "  SUCCESS - Accuracy: 0.7407, F1: 0.6716\n",
            "\n",
            "--- Training NEURAL_NETWORK on chunk 4 ---\n",
            "  SUCCESS - Accuracy: 0.0370, F1: 0.0026\n",
            "\n",
            "--- Training SVM on chunk 4 ---\n",
            "  SUCCESS - Accuracy: 0.8889, F1: 0.8778\n",
            "\n",
            "--- Training LASSO_REGRESSION on chunk 4 ---\n",
            "  SUCCESS - Accuracy: 0.1111, F1: 0.0357\n",
            "\n",
            "============================================================\n",
            "TRAINING ALL 5 MODELS ON CHUNK 5\n",
            "============================================================\n",
            "\n",
            "--- Training LOGISTIC_REGRESSION on chunk 5 ---\n",
            "  SUCCESS - Accuracy: 1.0000, F1: 1.0000\n",
            "\n",
            "--- Training DECISION_TREE on chunk 5 ---\n",
            "  SUCCESS - Accuracy: 1.0000, F1: 1.0000\n",
            "\n",
            "--- Training NEURAL_NETWORK on chunk 5 ---\n",
            "  SUCCESS - Accuracy: 0.1111, F1: 0.0222\n",
            "  NEW BEST - Best chunk for neural_network: 5 (f1_score: 0.0222)\n",
            "\n",
            "--- Training SVM on chunk 5 ---\n",
            "  SUCCESS - Accuracy: 0.9259, F1: 0.9189\n",
            "\n",
            "--- Training LASSO_REGRESSION on chunk 5 ---\n",
            "  SUCCESS - Accuracy: 0.1111, F1: 0.0647\n",
            "\n",
            "============================================================\n",
            "TRAINING ALL 5 MODELS ON CHUNK 6\n",
            "============================================================\n",
            "\n",
            "--- Training LOGISTIC_REGRESSION on chunk 6 ---\n",
            "  SUCCESS - Accuracy: 1.0000, F1: 1.0000\n",
            "\n",
            "--- Training DECISION_TREE on chunk 6 ---\n",
            "  SUCCESS - Accuracy: 0.8889, F1: 0.8519\n",
            "\n",
            "--- Training NEURAL_NETWORK on chunk 6 ---\n",
            "  SUCCESS - Accuracy: 0.1111, F1: 0.0222\n",
            "\n",
            "--- Training SVM on chunk 6 ---\n",
            "  SUCCESS - Accuracy: 1.0000, F1: 1.0000\n",
            "\n",
            "--- Training LASSO_REGRESSION on chunk 6 ---\n",
            "  SUCCESS - Accuracy: 0.0370, F1: 0.0026\n",
            "\n",
            "============================================================\n",
            "TRAINING ALL 5 MODELS ON CHUNK 7\n",
            "============================================================\n",
            "\n",
            "--- Training LOGISTIC_REGRESSION on chunk 7 ---\n",
            "  SUCCESS - Accuracy: 1.0000, F1: 1.0000\n",
            "\n",
            "--- Training DECISION_TREE on chunk 7 ---\n",
            "  SUCCESS - Accuracy: 0.7407, F1: 0.6716\n",
            "\n",
            "--- Training NEURAL_NETWORK on chunk 7 ---\n",
            "  SUCCESS - Accuracy: 0.0741, F1: 0.0102\n",
            "\n",
            "--- Training SVM on chunk 7 ---\n",
            "  SUCCESS - Accuracy: 0.9630, F1: 0.9613\n",
            "\n",
            "--- Training LASSO_REGRESSION on chunk 7 ---\n",
            "  SUCCESS - Accuracy: 0.0741, F1: 0.0275\n",
            "\n",
            "============================================================\n",
            "TRAINING ALL 5 MODELS ON CHUNK 8\n",
            "============================================================\n",
            "\n",
            "--- Training LOGISTIC_REGRESSION on chunk 8 ---\n",
            "  SUCCESS - Accuracy: 1.0000, F1: 1.0000\n",
            "\n",
            "--- Training DECISION_TREE on chunk 8 ---\n",
            "  SUCCESS - Accuracy: 0.7037, F1: 0.6402\n",
            "\n",
            "--- Training NEURAL_NETWORK on chunk 8 ---\n",
            "  SUCCESS - Accuracy: 0.1111, F1: 0.0222\n",
            "\n",
            "--- Training SVM on chunk 8 ---\n",
            "  SUCCESS - Accuracy: 1.0000, F1: 1.0000\n",
            "\n",
            "--- Training LASSO_REGRESSION on chunk 8 ---\n",
            "  SUCCESS - Accuracy: 0.1111, F1: 0.0769\n",
            "  NEW BEST - Best chunk for lasso_regression: 8 (f1_score: 0.0769)\n",
            "\n",
            "============================================================\n",
            "TRAINING ALL 5 MODELS ON CHUNK 9\n",
            "============================================================\n",
            "\n",
            "--- Training LOGISTIC_REGRESSION on chunk 9 ---\n",
            "  SUCCESS - Accuracy: 1.0000, F1: 1.0000\n",
            "\n",
            "--- Training DECISION_TREE on chunk 9 ---\n",
            "  SUCCESS - Accuracy: 0.7778, F1: 0.7500\n",
            "\n",
            "--- Training NEURAL_NETWORK on chunk 9 ---\n",
            "  SUCCESS - Accuracy: 0.0370, F1: 0.0027\n",
            "\n",
            "--- Training SVM on chunk 9 ---\n",
            "  SUCCESS - Accuracy: 1.0000, F1: 1.0000\n",
            "\n",
            "--- Training LASSO_REGRESSION on chunk 9 ---\n",
            "  SUCCESS - Accuracy: 0.0370, F1: 0.0026\n",
            "\n",
            "============================================================\n",
            "TRAINING ALL 5 MODELS ON CHUNK 10\n",
            "============================================================\n",
            "\n",
            "--- Training LOGISTIC_REGRESSION on chunk 10 ---\n",
            "  SUCCESS - Accuracy: 1.0000, F1: 1.0000\n",
            "\n",
            "--- Training DECISION_TREE on chunk 10 ---\n",
            "  SUCCESS - Accuracy: 0.8519, F1: 0.8148\n",
            "\n",
            "--- Training NEURAL_NETWORK on chunk 10 ---\n",
            "  SUCCESS - Accuracy: 0.0370, F1: 0.0028\n",
            "\n",
            "--- Training SVM on chunk 10 ---\n",
            "  SUCCESS - Accuracy: 1.0000, F1: 1.0000\n",
            "\n",
            "--- Training LASSO_REGRESSION on chunk 10 ---\n",
            "  SUCCESS - Accuracy: 0.1111, F1: 0.0476\n",
            "\n",
            "Saved 50 chunk models across 5 model types\n",
            "\n",
            "==================================================\n",
            "FINDING BEST CHUNK MODELS FOR EACH MODEL TYPE\n",
            "==================================================\n",
            "SUCCESS - logistic_regression  | Best chunk:  1 | f1_score: 1.0000 | Accuracy: 1.0000\n",
            "SUCCESS - decision_tree        | Best chunk:  2 | f1_score: 1.0000 | Accuracy: 1.0000\n",
            "SUCCESS - neural_network       | Best chunk:  5 | f1_score: 0.0222 | Accuracy: 0.1111\n",
            "SUCCESS - svm                  | Best chunk:  1 | f1_score: 1.0000 | Accuracy: 1.0000\n",
            "SUCCESS - lasso_regression     | Best chunk:  8 | f1_score: 0.0769 | Accuracy: 0.1111\n",
            "\n",
            "============================================================\n",
            "MODEL RANKING (by f1_score)\n",
            "============================================================\n",
            " 1. logistic_regression  | Avg f1_score: 1.0000 (±0.0000) | Best: 1.0000 (chunk 1) | Chunks: 10\n",
            " 2. svm                  | Avg f1_score: 0.9758 (±0.0414) | Best: 1.0000 (chunk 1) | Chunks: 10\n",
            " 3. decision_tree        | Avg f1_score: 0.7866 (±0.1339) | Best: 1.0000 (chunk 2) | Chunks: 10\n",
            " 4. lasso_regression     | Avg f1_score: 0.0337 (±0.0293) | Best: 0.0769 (chunk 8) | Chunks: 10\n",
            " 5. neural_network       | Avg f1_score: 0.0108 (±0.0081) | Best: 0.0222 (chunk 5) | Chunks: 10\n",
            "\n",
            "OVERALL BEST MODEL SELECTED: logistic_regression\n",
            "   Best chunk: 1\n",
            "   f1_score: 1.0000\n",
            "   Saved to: best_models/overall_best_model.pkl\n",
            "\n",
            "PIPELINE COMPLETED SUCCESSFULLY!\n",
            "   Models saved in: models/\n",
            "   Best models saved in: best_models/\n",
            "   Total chunks processed: 10\n",
            "   Total models trained: 5\n",
            "\n",
            "FINAL RESULTS SUMMARY:\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Chunk Trainer - Complete Machine Learning Pipeline\n",
        "\n",
        "Author: Prem Kumar Subudhi\n",
        "Date: 11-Oct-2025\n",
        "Version: 1.0\n",
        "\n",
        "Description:\n",
        "This script implements an advanced chunk-based training pipeline for multiclass classification.\n",
        "It trains multiple models across data chunks, evaluates performance, and selects the best\n",
        "performing model for deployment.\n",
        "\n",
        "Processing Steps:\n",
        "1. CONFIGURATION LOADING: Loads training parameters and model configurations from YAML file\n",
        "2. DATA INGESTION: Reads balanced dataset and auto-detects feature/label columns\n",
        "3. FEATURE ENGINEERING: Applies TF-IDF vectorization to transform text data into numerical features\n",
        "4. MODEL INITIALIZATION: Prepares 5 different classification algorithms with configured hyperparameters\n",
        "5. DATA CHUNKING: Partitions dataset into multiple chunks for distributed training\n",
        "6. PARALLEL TRAINING: Trains all models across all chunks with performance tracking\n",
        "7. MODEL PERSISTENCE: Saves all trained models with associated preprocessing components\n",
        "8. PERFORMANCE EVALUATION: Ranks models based on primary metrics and selects optimal performer\n",
        "9. MODEL SELECTION: Identifies and saves the best overall model for inference\n",
        "\n",
        "Dependencies:\n",
        "- pandas, numpy, scikit-learn, pyyaml\n",
        "\"\"\"\n",
        "\n",
        "# ChunkTrainer.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import yaml\n",
        "from datetime import datetime\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression # Added import\n",
        "from sklearn.tree import DecisionTreeClassifier # Added import\n",
        "from sklearn.svm import SVC # Added import\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class ChunkTrainer:\n",
        "    def __init__(self, config_path=\"config.yaml\"):\n",
        "        self.config = self.load_config(config_path)\n",
        "        self.balanced_df = None\n",
        "        self.chunks = []\n",
        "        self.models = {\n",
        "            'logistic_regression': {},\n",
        "            'decision_tree': {},\n",
        "            'neural_network': {},\n",
        "            'svm': {},\n",
        "            'lasso_regression': {}\n",
        "        }\n",
        "        self.metrics = {}\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.vectorizer = None\n",
        "        self.feature_columns = None\n",
        "        self.best_chunk_models = {}\n",
        "        self.overall_best_model = None\n",
        "\n",
        "    def load_config(self, config_path):\n",
        "        \"\"\"Load configuration from YAML file\"\"\"\n",
        "        try:\n",
        "            with open(config_path, 'r') as file:\n",
        "                config = yaml.safe_load(file)\n",
        "            print(f\"Configuration loaded successfully from {config_path}\")\n",
        "            return config\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading config from {config_path}: {e}\")\n",
        "            return self.get_default_config()\n",
        "\n",
        "    def get_default_config(self):\n",
        "        \"\"\"Provide default configuration\"\"\"\n",
        "        return {\n",
        "            'models': {\n",
        "                'logistic_regression': {\n",
        "                    'multi_class': 'multinomial', 'solver': 'lbfgs', 'max_iter': 1000, 'random_state': 42, 'class_weight': 'balanced'\n",
        "                },\n",
        "                'decision_tree': {\n",
        "                    'max_depth': 10, 'random_state': 42, 'class_weight': 'balanced'\n",
        "                },\n",
        "                'neural_network': {\n",
        "                    'hidden_layer_sizes': [100, 50], 'max_iter': 1000, 'random_state': 42, 'early_stopping': True\n",
        "                },\n",
        "                'svm': {\n",
        "                    'kernel': 'linear', 'probability': True, 'random_state': 42, 'class_weight': 'balanced'\n",
        "                },\n",
        "                'lasso_regression': {\n",
        "                    'multi_class': 'multinomial', 'solver': 'saga', 'max_iter': 1000, 'random_state': 42, 'class_weight': 'balanced', 'penalty': 'l1'\n",
        "                }\n",
        "            },\n",
        "            'training': {\n",
        "                'num_chunks': 10, 'random_state': 42, 'primary_metric': 'f1_score'\n",
        "            },\n",
        "            'paths': {\n",
        "                'balanced_dataset': \"balanced_dataset\", 'chunks_dir': \"chunks\",\n",
        "                'models_dir': \"models\", 'best_models_dir': \"best_models\", 'reports_dir': \"reports\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "\n",
        "    def load_balanced_dataset(self):\n",
        "        \"\"\"Load the balanced dataset\"\"\"\n",
        "        balanced_path = self.config['paths']['balanced_dataset']\n",
        "        try:\n",
        "            # Try loading from CSV\n",
        "            csv_path = os.path.join(balanced_path, \"balanced_dataset.csv\")\n",
        "            if os.path.exists(csv_path):\n",
        "                self.balanced_df = pd.read_csv(csv_path)\n",
        "                print(f\"Loaded balanced dataset from: {csv_path}\")\n",
        "                print(f\"Dataset shape: {self.balanced_df.shape}\")\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"Balanced dataset not found at: {csv_path}\")\n",
        "                return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading balanced dataset: {e}\")\n",
        "            return False\n",
        "\n",
        "    def _auto_detect_label_column(self):\n",
        "        \"\"\"Auto-detect the label column\"\"\"\n",
        "        label_candidates = ['label', 'category', 'class', 'target', 'industry', 'industry_name']\n",
        "        for candidate in label_candidates:\n",
        "            if candidate in self.balanced_df.columns:\n",
        "                return candidate\n",
        "        return self.balanced_df.columns[0] if len(self.balanced_df.columns) > 0 else None\n",
        "\n",
        "    def _auto_detect_text_column(self):\n",
        "        \"\"\"Auto-detect the text column\"\"\"\n",
        "        text_candidates = ['text', 'content', 'summary', 'description', 'full_summary']\n",
        "        for candidate in text_candidates:\n",
        "            if candidate in self.balanced_df.columns:\n",
        "                return candidate\n",
        "        for col in self.balanced_df.columns:\n",
        "            if self.balanced_df[col].dtype == 'object' and self.balanced_df[col].str.len().mean() > 10:\n",
        "                return col\n",
        "        return self.balanced_df.columns[0] if len(self.balanced_df.columns) > 0 else None\n",
        "\n",
        "    def prepare_features(self):\n",
        "        \"\"\"Prepare features using TF-IDF\"\"\"\n",
        "        text_column = self._auto_detect_text_column()\n",
        "        label_column = self._auto_detect_label_column()\n",
        "\n",
        "        if not text_column or not label_column:\n",
        "            print(\"Could not auto-detect text or label column\")\n",
        "            print(f\"Available columns: {self.balanced_df.columns.tolist()}\")\n",
        "            return False\n",
        "\n",
        "        self.text_column = text_column\n",
        "        self.label_column = label_column\n",
        "\n",
        "        print(f\"Using text column: '{text_column}'\")\n",
        "        print(f\"Using label column: '{label_column}'\")\n",
        "\n",
        "        # Get TF-IDF parameters from config\n",
        "        tfidf_config = self.config.get('feature_engineering', {}).get('tfidf', {\n",
        "            'max_features': 1000,\n",
        "            'stop_words': 'english',\n",
        "            'ngram_range': [1, 2],\n",
        "            'min_df': 1,\n",
        "            'max_df': 1.0\n",
        "        })\n",
        "\n",
        "        texts = self.balanced_df[text_column].fillna('').astype(str)\n",
        "\n",
        "        self.vectorizer = TfidfVectorizer(\n",
        "            max_features=tfidf_config['max_features'],\n",
        "            stop_words=tfidf_config['stop_words'],\n",
        "            ngram_range=tuple(tfidf_config['ngram_range']),\n",
        "            min_df=tfidf_config['min_df'],\n",
        "            max_df=tfidf_config['max_df']\n",
        "        )\n",
        "\n",
        "\n",
        "        X = self.vectorizer.fit_transform(texts)\n",
        "        self.feature_columns = self.vectorizer.get_feature_names_out()\n",
        "\n",
        "        # Prepare labels\n",
        "        y = self.balanced_df[label_column]\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        y_encoded = self.label_encoder.fit_transform(y)\n",
        "\n",
        "\n",
        "        self.X = X\n",
        "        self.y = y_encoded\n",
        "\n",
        "        print(f\"Created features with shape: {X.shape}\")\n",
        "        print(f\"Number of classes: {len(self.label_encoder.classes_)}\")\n",
        "        print(f\"Classes: {list(self.label_encoder.classes_)}\")\n",
        "\n",
        "\n",
        "        return True\n",
        "\n",
        "\n",
        "    def initialize_models(self):\n",
        "        \"\"\"Initialize all 5 models with parameters from config\"\"\"\n",
        "        models_config = self.config['models']\n",
        "\n",
        "        # Logistic Regression\n",
        "        lr_config = models_config['logistic_regression']\n",
        "        self.models['logistic_regression'] = {\n",
        "            'model_class': LogisticRegression,\n",
        "            'config': lr_config,\n",
        "            'chunk_models': {},\n",
        "            'best_chunk': None,\n",
        "            'best_score': -1\n",
        "        }\n",
        "\n",
        "        # Decision Tree\n",
        "        dt_config = models_config['decision_tree']\n",
        "        self.models['decision_tree'] = {\n",
        "            'model_class': DecisionTreeClassifier,\n",
        "            'config': dt_config,\n",
        "            'chunk_models': {},\n",
        "            'best_chunk': None,\n",
        "            'best_score': -1\n",
        "        }\n",
        "\n",
        "        # Neural Network\n",
        "        nn_config = models_config['neural_network']\n",
        "        if 'hidden_layer_sizes' in nn_config and isinstance(nn_config['hidden_layer_sizes'], list):\n",
        "            nn_config['hidden_layer_sizes'] = tuple(nn_config['hidden_layer_sizes'])\n",
        "        self.models['neural_network'] = {\n",
        "            'model_class': MLPClassifier,\n",
        "            'config': nn_config,\n",
        "            'chunk_models': {},\n",
        "            'best_chunk': None,\n",
        "            'best_score': -1\n",
        "        }\n",
        "\n",
        "        # SVM\n",
        "        svm_config = models_config['svm']\n",
        "        self.models['svm'] = {\n",
        "            'model_class': SVC,\n",
        "            'config': svm_config,\n",
        "            'chunk_models': {},\n",
        "            'best_chunk': None,\n",
        "            'best_score': -1\n",
        "        }\n",
        "\n",
        "        # Lasso Regression (Logistic Regression with L1 penalty)\n",
        "        lasso_config = models_config['lasso_regression']\n",
        "        self.models['lasso_regression'] = {\n",
        "            'model_class': LogisticRegression,\n",
        "            'config': lasso_config,\n",
        "            'chunk_models': {},\n",
        "            'best_chunk': None,\n",
        "            'best_score': -1\n",
        "        }\n",
        "\n",
        "        print(\"Initialized 5 models for multiclass classification:\")\n",
        "        for model_name in self.models.keys():\n",
        "            print(f\"  - {model_name}\")\n",
        "\n",
        "        return self.models\n",
        "\n",
        "    def create_chunks(self):\n",
        "        \"\"\"Create chunks from the balanced dataset\"\"\"\n",
        "        num_chunks = self.config['training']['num_chunks']\n",
        "\n",
        "        if not hasattr(self, 'X') or self.X is None:\n",
        "            print(\"Features not prepared.\")\n",
        "            return []\n",
        "\n",
        "        total_samples = self.X.shape[0]\n",
        "        chunk_size = total_samples // num_chunks\n",
        "        remaining = total_samples % num_chunks\n",
        "\n",
        "        print(f\"\\nCreating {num_chunks} chunks from {total_samples} samples...\")\n",
        "\n",
        "        self.chunks = []\n",
        "        start_idx = 0\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            current_chunk_size = chunk_size + (1 if i < remaining else 0)\n",
        "            end_idx = start_idx + current_chunk_size\n",
        "\n",
        "            chunk_data = {\n",
        "                'chunk_id': i + 1,\n",
        "                'start_idx': start_idx,\n",
        "                'end_idx': end_idx,\n",
        "                'size': current_chunk_size,\n",
        "                'X': self.X[start_idx:end_idx],\n",
        "                'y': self.y[start_idx:end_idx],\n",
        "                'class_distribution': dict(pd.Series(self.y[start_idx:end_idx]).value_counts())\n",
        "            }\n",
        "\n",
        "            self.chunks.append(chunk_data)\n",
        "            print(f\"Chunk {i+1}: {current_chunk_size} samples, {len(chunk_data['class_distribution'])} classes\")\n",
        "\n",
        "            start_idx = end_idx\n",
        "\n",
        "\n",
        "        return self.chunks\n",
        "\n",
        "    def train_models_on_chunk(self, chunk):\n",
        "        \"\"\"Train all 5 models on a specific chunk\"\"\"\n",
        "        chunk_id = chunk['chunk_id']\n",
        "        X_chunk = chunk['X']\n",
        "        y_chunk = chunk['y']\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"TRAINING ALL 5 MODELS ON CHUNK {chunk_id}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        chunk_metrics = {}\n",
        "\n",
        "        for model_name, model_info in self.models.items():\n",
        "            print(f\"\\n--- Training {model_name.upper()} on chunk {chunk_id} ---\")\n",
        "\n",
        "            try:\n",
        "                # Create fresh model instance\n",
        "                model_instance = model_info['model_class'](**model_info['config'])\n",
        "\n",
        "                # Train the model\n",
        "                model_instance.fit(X_chunk, y_chunk)\n",
        "\n",
        "                # Make predictions\n",
        "                y_pred = model_instance.predict(X_chunk)\n",
        "\n",
        "                # Calculate metrics\n",
        "                accuracy = accuracy_score(y_chunk, y_pred)\n",
        "                f1 = f1_score(y_chunk, y_pred, average='weighted')\n",
        "                precision = precision_score(y_chunk, y_pred, average='weighted', zero_division=0)\n",
        "                recall = recall_score(y_chunk, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "                metrics = {\n",
        "                    'accuracy': float(accuracy),\n",
        "                    'f1_score': float(f1),\n",
        "                    'precision': float(precision),\n",
        "                    'recall': float(recall),\n",
        "                    'train_samples': X_chunk.shape[0]\n",
        "                }\n",
        "\n",
        "                # Store the trained model with vectorizer and encoder\n",
        "                model_info['chunk_models'][chunk_id] = {\n",
        "                    'model': model_instance,\n",
        "                    'metrics': metrics,\n",
        "                    'trained_at': datetime.now().isoformat(),\n",
        "                    'vectorizer': self.vectorizer,  # Save vectorizer\n",
        "                    'label_encoder': self.label_encoder,  # Save label encoder\n",
        "                    'X': X_chunk,  # Save training data for drift analysis\n",
        "                    'y': y_chunk   # Save labels for drift analysis\n",
        "                }\n",
        "\n",
        "                chunk_metrics[model_name] = metrics\n",
        "\n",
        "                print(f\"  SUCCESS - Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "                # Update best chunk for this model\n",
        "                primary_metric = self.config['training']['primary_metric']\n",
        "                current_score = metrics[primary_metric]\n",
        "                if current_score > model_info['best_score']:\n",
        "                    model_info['best_score'] = current_score\n",
        "                    model_info['best_chunk'] = chunk_id\n",
        "                    print(f\"  NEW BEST - Best chunk for {model_name}: {chunk_id} ({primary_metric}: {current_score:.4f})\")\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  ERROR training {model_name}: {e}\")\n",
        "                chunk_metrics[model_name] = {'error': str(e)}\n",
        "\n",
        "\n",
        "        self.metrics[chunk_id] = chunk_metrics\n",
        "        return chunk_metrics\n",
        "\n",
        "\n",
        "    def save_all_chunk_models(self):\n",
        "        \"\"\"Save all models for all chunks with vectorizer and encoder\"\"\"\n",
        "        models_dir = self.config['paths']['models_dir']\n",
        "        os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "        total_saved = 0\n",
        "        for model_name, model_info in self.models.items():\n",
        "            model_dir = os.path.join(models_dir, model_name)\n",
        "            os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "            for chunk_id, chunk_model in model_info['chunk_models'].items():\n",
        "                filename = os.path.join(model_dir, f\"chunk_{chunk_id}.pkl\")\n",
        "                try:\n",
        "                    with open(filename, 'wb') as f:\n",
        "                        pickle.dump(chunk_model, f)\n",
        "                    total_saved += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"Error saving {model_name} chunk {chunk_id}: {e}\")\n",
        "\n",
        "        print(f\"\\nSaved {total_saved} chunk models across 5 model types\")\n",
        "        return total_saved\n",
        "\n",
        "    def find_and_save_best_chunk_models(self):\n",
        "        \"\"\"Find and save the best chunk model for each model type\"\"\"\n",
        "        best_models_dir = self.config['paths']['best_models_dir']\n",
        "        os.makedirs(best_models_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(\"FINDING BEST CHUNK MODELS FOR EACH MODEL TYPE\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        self.best_chunk_models = {}\n",
        "\n",
        "        for model_name, model_info in self.models.items():\n",
        "            best_chunk = model_info['best_chunk']\n",
        "            if best_chunk and best_chunk in model_info['chunk_models']:\n",
        "                best_model_data = model_info['chunk_models'][best_chunk]\n",
        "                self.best_chunk_models[model_name] = best_model_data\n",
        "\n",
        "                # Save best model\n",
        "                filename = os.path.join(best_models_dir, f\"best_{model_name}.pkl\")\n",
        "                try:\n",
        "                    with open(filename, 'wb') as f:\n",
        "                        pickle.dump(best_model_data, f)\n",
        "\n",
        "                    metrics = best_model_data['metrics']\n",
        "                    primary_metric = self.config['training']['primary_metric']\n",
        "                    primary_metric_value = metrics[primary_metric]\n",
        "                    print(f\"SUCCESS - {model_name:20} | Best chunk: {best_chunk:2} | {primary_metric}: {primary_metric_value:.4f} | Accuracy: {metrics['accuracy']:.4f}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error saving best {model_name}: {e}\")\n",
        "            else:\n",
        "                print(f\"FAILED - {model_name:20} | No best chunk found\")\n",
        "\n",
        "        return self.best_chunk_models\n",
        "\n",
        "\n",
        "    def rank_models(self):\n",
        "        \"\"\"Rank all models based on performance metrics\"\"\"\n",
        "        if not self.best_chunk_models:\n",
        "            print(\"No best models found for ranking\")\n",
        "            return []\n",
        "\n",
        "        primary_metric = self.config['training']['primary_metric']\n",
        "\n",
        "        # Calculate average metrics across all chunks for each model\n",
        "        model_scores = []\n",
        "\n",
        "        for model_name in self.models.keys():\n",
        "            model_metrics = []\n",
        "            for chunk_id, chunk_metrics in self.metrics.items():\n",
        "                if model_name in chunk_metrics and primary_metric in chunk_metrics[model_name]:\n",
        "                    model_metrics.append(chunk_metrics[model_name][primary_metric])\n",
        "\n",
        "            if model_metrics:\n",
        "                avg_score = np.mean(model_metrics)\n",
        "                std_score = np.std(model_metrics)\n",
        "\n",
        "                # Get best chunk performance\n",
        "                best_score = self.models[model_name]['best_score']\n",
        "                best_chunk = self.models[model_name]['best_chunk']\n",
        "\n",
        "                model_scores.append({\n",
        "                    'model': model_name,\n",
        "                    f'avg_{primary_metric}': avg_score,\n",
        "                    f'std_{primary_metric}': std_score,\n",
        "                    f'best_{primary_metric}': best_score,\n",
        "                    'best_chunk': best_chunk,\n",
        "                    'chunks_trained': len(model_metrics)\n",
        "                })\n",
        "\n",
        "\n",
        "        # Sort by primary metric (descending)\n",
        "        model_scores.sort(key=lambda x: x[f'avg_{primary_metric}'], reverse=True)\n",
        "\n",
        "\n",
        "        return model_scores\n",
        "\n",
        "\n",
        "    def print_model_ranking(self):\n",
        "        \"\"\"Print ranked list of models\"\"\"\n",
        "        model_ranking = self.rank_models()\n",
        "        primary_metric = self.config['training']['primary_metric']\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"MODEL RANKING (by {primary_metric})\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        for i, model_data in enumerate(model_ranking, 1):\n",
        "            avg_metric = model_data[f'avg_{primary_metric}']\n",
        "            std_metric = model_data[f'std_{primary_metric}']\n",
        "            best_metric = model_data[f'best_{primary_metric}']\n",
        "            best_chunk = model_data['best_chunk']\n",
        "            chunks_trained = model_data['chunks_trained']\n",
        "\n",
        "            print(f\"{i:2}. {model_data['model']:20} | \"\n",
        "                  f\"Avg {primary_metric}: {avg_metric:.4f} (±{std_metric:.4f}) | \"\n",
        "                  f\"Best: {best_metric:.4f} (chunk {best_chunk}) | \"\n",
        "                  f\"Chunks: {chunks_trained}\")\n",
        "\n",
        "\n",
        "        return model_ranking\n",
        "\n",
        "\n",
        "    def select_and_save_overall_best_model(self):\n",
        "        \"\"\"Select and save the overall best model for future inference\"\"\"\n",
        "        model_ranking = self.rank_models()\n",
        "\n",
        "        if not model_ranking:\n",
        "            print(\"No models available for selection\")\n",
        "            return None\n",
        "\n",
        "        best_model_data = model_ranking[0]\n",
        "        best_model_name = best_model_data['model']\n",
        "        best_chunk = self.models[best_model_name]['best_chunk']\n",
        "\n",
        "        if best_chunk in self.models[best_model_name]['chunk_models']:\n",
        "            self.overall_best_model = self.models[best_model_name]['chunk_models'][best_chunk]\n",
        "\n",
        "            # Save overall best model\n",
        "            best_models_dir = self.config['paths']['best_models_dir']\n",
        "            filename = os.path.join(best_models_dir, \"overall_best_model.pkl\")\n",
        "\n",
        "            try:\n",
        "                # Extract the primary metric value safely\n",
        "                primary_metric_name = self.config['training']['primary_metric']\n",
        "                best_metric_value = best_model_data[f'best_{primary_metric_name}']\n",
        "\n",
        "\n",
        "                with open(filename, 'wb') as f:\n",
        "                    pickle.dump({\n",
        "                        'model': self.overall_best_model['model'],\n",
        "                        'model_name': best_model_name,\n",
        "                        'best_chunk': best_chunk,\n",
        "                        'metrics': self.overall_best_model['metrics'],\n",
        "                        'primary_metric': primary_metric_name,\n",
        "                        'selected_at': datetime.now().isoformat(),\n",
        "                        'feature_columns': self.feature_columns.tolist() if hasattr(self, 'feature_columns') else [],\n",
        "                        'label_encoder': self.label_encoder,\n",
        "                        'vectorizer': self.vectorizer\n",
        "                    }, f)\n",
        "\n",
        "                print(f\"\\nOVERALL BEST MODEL SELECTED: {best_model_name}\")\n",
        "                print(f\"   Best chunk: {best_chunk}\")\n",
        "                print(f\"   {primary_metric_name}: {best_metric_value:.4f}\")\n",
        "                print(f\"   Saved to: {filename}\")\n",
        "\n",
        "\n",
        "                return self.overall_best_model\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error saving overall best model: {e}\")\n",
        "                return None\n",
        "\n",
        "        return None\n",
        "\n",
        "\n",
        "    def run_complete_pipeline(self):\n",
        "        \"\"\"Run the complete training pipeline\"\"\"\n",
        "        print(\"STARTING COMPLETE TRAINING PIPELINE\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Step 1: Load balanced dataset\n",
        "        if not self.load_balanced_dataset():\n",
        "            return False\n",
        "\n",
        "        # Step 2: Prepare features\n",
        "        if not self.prepare_features():\n",
        "            return False\n",
        "\n",
        "        # Step 3: Create chunks\n",
        "        self.create_chunks()\n",
        "\n",
        "        # Step 4: Initialize models\n",
        "        self.initialize_models()\n",
        "\n",
        "        # Step 5: Train on all chunks\n",
        "        for chunk in self.chunks:\n",
        "            self.train_models_on_chunk(chunk)\n",
        "\n",
        "        # Step 6: Save all chunk models\n",
        "        self.save_all_chunk_models()\n",
        "\n",
        "        # Step 7: Find and save best chunk models\n",
        "        self.find_and_save_best_chunk_models()\n",
        "\n",
        "        # Step 8: Rank models and print results\n",
        "        self.print_model_ranking()\n",
        "\n",
        "        # Step 9: Select and save overall best model\n",
        "        self.select_and_save_overall_best_model()\n",
        "\n",
        "        print(f\"\\nPIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "        print(f\"   Models saved in: {self.config['paths']['models_dir']}/\")\n",
        "        print(f\"   Best models saved in: {self.config['paths']['best_models_dir']}/\")\n",
        "        print(f\"   Total chunks processed: {len(self.chunks)}\")\n",
        "        print(f\"   Total models trained: {len(self.models)}\")\n",
        "\n",
        "\n",
        "        return True\n",
        "\n",
        "\n",
        "def main():\n",
        "    trainer = ChunkTrainer(\"config.yaml\")\n",
        "    success = trainer.run_complete_pipeline()\n",
        "\n",
        "    if success:\n",
        "        print(\"\\nFINAL RESULTS SUMMARY:\")\n",
        "        print(\"=\" * 40)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edbd1718-3d1b-4fa3-9ccd-23f1d6c24ca2",
      "metadata": {
        "id": "edbd1718-3d1b-4fa3-9ccd-23f1d6c24ca2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "conda_python3",
      "language": "python",
      "name": "conda_python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}